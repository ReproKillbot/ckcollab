---
title:  "Optimizing Heroku Django Memory"
date: 2019-01-25 00:00:00 +0800
permalink: /test/
---


This week we're writing about a particularly tech detailed situation we have with a client. The
architecture that has slowed down and started using too much memory. We'll talk about how we're
attempting to alleviate the problem.


# What is happening?

Currently, the client has the following arguments being passed to [waitress](https://docs.pylonsproject.org/projects/waitress/en/latest/):

    waitress-serve --threads=8 --port=$PORT --send-bytes=50 wsgi:application


This tells waitress to spawn 8 threads per worker on a port Heroku passes us, and clear the buffer after
50 bytes have been queued for sending. The reason
for this small queue is to fix a problem with Heroku and not sending out streaming responses until
a buffer is filled. Also, default worker count in gunicorn is `(2 * CPU_COUNT) + 1` for reference.

The problem, we can see from these nice Heroku analytics, is (a) memory usage growing too high over time
and (b) responses hitting the 30s time limit Heroku allows for generating a response.

<img src="/assets/images/articles/heroku_memory_1.png" class="img-bordered">

# First problem: memory

### Why are we using so much memory?

Probably because we're using 8 threads. This application is probably much more IO bound than CPU bound, so
using threads makes sense, but in our case it's probably just plain-too-many.

### How to fix?

To fix this problem, I'll spawn less threads so we should use much less memory. I found [this](https://devcenter.heroku.com/articles/optimizing-dyno-usage#python)
handy document from Heroku giving us a suggested amount of workers per dyno. We'll go with 3 for now to see
if that stops memory usage problems.



# Second problem: timeouts

I bet the source of timeouts is probably an API request generating way too many queries to Postgres -- or -- potentially
some event-triggered-by-another-event is firing out of sync and forcing something to wait indefinitely.

To start investigating this problem, I had to switch our staging server into a "debug mode" which allowed me to print out
all of the SQL queries that could be causing problems. Then instantly I see some culprits: `/api/orders` is generating
176 queries, with lots of duplicates that could be potentially avoided:

<img src="/assets/images/articles/heroku_memory_2.png" class="img-bordered">

The offending query is generated by our [django-rest-framework](https://www.django-rest-framework.org/) serializer

```

class OrderSerializer(serializers.ModelSerializer):

    class Meta:
        model = Order
        fields = (
            'date',
            'office',
            ...
        )

```

Typically the fix is to use [`select_related`](https://docs.djangoproject.com/en/2.1/ref/models/querysets/#select-related) and
 [`prefetch_related`](https://docs.djangoproject.com/en/2.1/ref/models/querysets/#prefetch-related)
 to grab all of the relevant data in a couple quick call to the database,
so we don't have to make 100+ subsequent queries.

However, this time the culprit seems to be just plain grabbing too much data back from the database. We were getting companies and
offices of each user who made an order, not just their name and address! **Query count plumetted from 176 to 42** in one go.

A couple more fixes (a nice use of `select_related`) and we're down from 42 to... 4! Very acceptable.

In another instance I added another flag and dropped queries from 511 to 4.


# Common slow downs in Django

These common things didn't end up slowing down the server in this case, but do come up often:

1. How are you serving static files? Make sure you're using [whitenoise](http://whitenoise.evans.io/en/stable/) with Django on Heroku.
2. Make sure you are leveraging some kind of caching, like [Redis](https://redis.io/)
3. Make sure your static assets are marking themselves to be cached
4. Serve from a CDN when possible (although this won't cause a Heroku timeout, a good tip!)


# Conclusion

Thanks for following along, it's always fun to leverage the same amount of resources but get a quick 5x performance boost!
